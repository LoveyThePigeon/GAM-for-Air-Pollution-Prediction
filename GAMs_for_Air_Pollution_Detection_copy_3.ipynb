{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVLvMvzQB+oz8uEO1QC2vg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LoveyThePigeon/GAM-for-Air-Pollution-Prediction/blob/main/GAMs_for_Air_Pollution_Detection_copy_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# Data split into TRAIN AND TEST, contains new code and old commented out code\n",
        "# No changes in cell with findings\n",
        "###############################################################################"
      ],
      "metadata": {
        "id": "Iu8wOaV8bFk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. LOADING DATA FROM \"PM10 air pollution.xlsx\"\n",
        "\"\"\"\n",
        "Findings: 1095 rows, 11 columns, last column PM10 - y\n",
        "Checking for outliers - visible outliers, however this is weather data which is often unpredictable, so we are keeping all data\n",
        "\"\"\"\n",
        "# 2. # SPLITTING DATA 60% - TRAIN, 20% - VALIDATE, 20% - TEST\n",
        "\"\"\"\n",
        "Findings: Data split: train: 657, validate: 219, test: 219 rows\n",
        "\"\"\"\n",
        "# 3. PREPROCESSING\n",
        "\"\"\"\n",
        "Findings: Preprocessed with standard scaler\n",
        "\"\"\"\n",
        "# 4. CHECKING DISTRIBUTION FOR GAM\n",
        "\"\"\"\n",
        "Findings: Histogram shows tail on the right side, suggesting Gamma distribution\n",
        "          KDE function confirms tail and shows that train, validate, test behave the same way,\n",
        "          Q-Q plot of normal distribution is not flat, but raised above the line on both sides\n",
        "          skewness shows 5.72 - tail to the right (= means symmetric, > 0 tail to the right, < 0 tail to the left), all y values are cofirmed positive (requirement for Gamma)\n",
        "          all values of y are positive\n",
        "          Q-Q plot for Gamma distribution confirms that Gamma is better fit\n",
        "\"\"\"\n",
        "# 5. SETTING UP THE GAM MODEL, FITTING, TRAINING, CHECKING RESULTS\n",
        "\"\"\"\n",
        "Findings: Overall findings: Based on metrics for VALIDATION set, NORMAL distribution appears to be better choice\n",
        "\"\"\"\n",
        "## a. Normal\n",
        "\"\"\"\n",
        "Findings:\n",
        "===== For defaults: ======\n",
        "Validation MAE: 10.414243431780504\n",
        "Validation RMSE: 13.345215184369117\n",
        "Validation R¬≤: 0.45667658161922586\n",
        "\n",
        "Test MAE: 10.153362530604754\n",
        "Test RMSE: 14.375904927571652\n",
        "Test R¬≤: 0.4523946604236567\n",
        "\n",
        "===== Tuning: for lam = [35, 70] =====\n",
        "[[np.int64(70)], [np.int64(35)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)]]\n",
        "\n",
        "Validation MAE: 9.367897732609359\n",
        "Validation RMSE: 12.32485430135225\n",
        "Validation R¬≤: 0.5365841854479299 <===\n",
        "\n",
        "Test MAE: 9.328121702533512\n",
        "Test RMSE: 13.195446091358482\n",
        "Test R¬≤: 0.5386341515153062\n",
        "\"\"\"\n",
        "## b. Gamma\n",
        "\"\"\"\n",
        "Findings:\n",
        "===== For defaults =====\n",
        "Validation MAE: 10.106722495367285\n",
        "Validation RMSE: 13.565756054293036\n",
        "Validation R¬≤: 0.43857044812864543\n",
        "\n",
        "Test MAE: 9.301673017566701\n",
        "Test RMSE: 13.488316090536276\n",
        "Test R¬≤: 0.517927053577327\n",
        "\n",
        "===== Tuning: for lam = [35, 70] =====\n",
        "[[np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(35)], [np.int64(70)], [np.int64(70)]]\n",
        "\n",
        "Validation MAE: 9.578659857788022\n",
        "Validation RMSE: 12.841379189145238\n",
        "Validation R¬≤: 0.49692747044373253 <===\n",
        "\n",
        "Test MAE: 9.080696917870128\n",
        "Test RMSE: 12.880167750389928\n",
        "Test R¬≤: 0.5604175666292098\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# 6. LINEAR REGRESSION\n",
        "\"\"\"\n",
        "Findings:\n",
        "Defaults:\n",
        "Validation MAE: 9.863943254894515\n",
        "Validation RMSE: 13.414917594066392\n",
        "Validation R¬≤: 0.4509861747626672\n",
        "\n",
        "Test MAE: 9.364195884679772\n",
        "Test RMSE: 13.08551276609917\n",
        "Test R¬≤: 0.546289551047396\n",
        "\n",
        "\"\"\"\n",
        "# 7. RANDOM FOREST\n",
        "\"\"\"\n",
        "Findings:\n",
        "Validation MAE: 10.31728524543379\n",
        "Validation RMSE: 16.237391876373135\n",
        "Validation R¬≤: 0.19565974058557334\n",
        "\n",
        "Test MAE: 11.90439604737443\n",
        "Test RMSE: 20.968070060257354\n",
        "Test R¬≤: -0.16496856928987258\n",
        "\n",
        "After tuning:\n",
        "Validation MAE: 9.741866589464983\n",
        "Validation RMSE: 14.094400343057895\n",
        "Validation R¬≤: 0.3939611450312881\n",
        "\n",
        "Test MAE: 10.873283599074258\n",
        "Test RMSE: 16.550293405305123\n",
        "Test R¬≤: 0.27421383560963153\n",
        "\"\"\"\n",
        "# 7. OVERALL PERFORMANCE (BASED ON VALIDATION R^2)\n",
        "'''\n",
        "Best: Linear GAM\n",
        "      Gamma GAM\n",
        "      Linear Regression\n",
        "      Random Forest\n",
        "'''\n"
      ],
      "metadata": {
        "id": "4G788dZljleq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ils7OqijeGV"
      },
      "outputs": [],
      "source": [
        "####################################################\n",
        "# LOADING DATA FROM \"PM10 air pollution.xlsx\"\n",
        "####################################################\n",
        "# !pip install openpyxl\n",
        "from IPython.display import display\n",
        "\n",
        "# loads  data - comment/ uncomment next two lines to load the file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "import pandas as pd\n",
        "df = pd.read_excel(\"PM10 air pollution.xlsx\")\n",
        "#df = pd.read_csv(\"/content/PM10 air pollution.csv\")\n",
        "\n",
        "# displays sample data and other information\n",
        "print(\"\\n########## FIRST 5 ROWS ##########\\n\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\n########## DATA TYPES AND MISSING VALUES ##########\\n\")\n",
        "display(df.info())\n",
        "\n",
        "print(\"\\n########## SIZE OF THE DATASET ##########\\n\")\n",
        "display(df.shape)\n",
        "\n",
        "print(\"\\n########## BASIC STATISTICS ##########\\n\")\n",
        "display(df.describe())\n",
        "\n",
        "####################################################\n",
        "# OUTLIERS\n",
        "####################################################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "for col in df.columns:\n",
        "  sns.boxplot(x=df[col])\n",
        "  plt.title(f\"Boxplot of {col}\")\n",
        "  plt.show()\n",
        "\n",
        "'''\n",
        "####################################################\n",
        "# SPLITTING DATA 60% - TRAIN, 20% - VALIDATE, 20% - TEST\n",
        "####################################################\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.iloc[:,0:10].to_numpy() # X: all rows, 10 columns\n",
        "y = df.iloc[:,10].to_numpy() # y: all rows, last column\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=18) # splits data 60% for train, 40% will be used for validation and test half and half\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=18) # splits temp into validation and test half and half\n",
        "\n",
        "\n",
        "print(\"\\n########## SPLITTING ##########\\n\")\n",
        "print(\"Dimensions of X:\", X_train.ndim, X_val.ndim, X_test.ndim) # prints dimensions of X\n",
        "print(\"Dimensions of y:\", y_train.ndim, y_val.ndim, y_test.ndim) # prints dimensions of y\n",
        "\n",
        "print(\"Sizes X rows:\", X_train.shape[0], X_val.shape[0], X_test.shape[0]) # number of rows\n",
        "print(\"Sizes X columns:\", X_train.shape[1], X_val.shape[1], X_test.shape[1]) # number of columns\n",
        "\n",
        "print(\"Sizes y:\", y_train.shape[0], y_val.shape[0], y_test.shape[0]) # number of rows (this is an array, does not have number of columns like X)\n",
        "\n",
        "'''\n",
        "\n",
        "####################################################\n",
        "# SPLITTING DATA 80% - TRAIN, 20% - TEST\n",
        "####################################################\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.iloc[:,0:10].to_numpy() # X: all rows, 10 columns\n",
        "y = df.iloc[:,10].to_numpy() # y: all rows, last column\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=18) # splits data 80% for train, 20% will be used for test\n",
        "\n",
        "print(\"\\n########## SPLITTING ##########\\n\")\n",
        "print(\"Dimensions of X:\", X_train.ndim, X_test.ndim) # prints dimensions of X\n",
        "print(\"Dimensions of y:\", y_train.ndim, y_test.ndim) # prints dimensions of y\n",
        "\n",
        "print(\"Sizes X rows:\", X_train.shape[0], X_test.shape[0]) # number of rows\n",
        "print(\"Sizes X columns:\", X_train.shape[1], X_test.shape[1]) # number of columns\n",
        "\n",
        "print(\"Sizes y:\", y_train.shape[0], y_test.shape[0]) # number of rows (this is an array, does not have number of columns like X)\n",
        "\n",
        "####################################################\n",
        "# PREPROCESSING\n",
        "####################################################\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on X_train and transform all sets\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "# X_val   = scaler.transform(X_val)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "####################################################\n",
        "# CHECKING DISTRIBUTION FOR GAM\n",
        "####################################################\n",
        "\n",
        "print(\"\\n########## DISTRIBUTION ##########\\n\")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Histogram of y_train\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(y_train, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "\n",
        "plt.title(\"Histogram of y_train\")\n",
        "plt.xlabel(\"y values\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "# KDE function\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.kdeplot(y_train, label='Train')\n",
        "# sns.kdeplot(y_val, label='Validation')\n",
        "sns.kdeplot(y_test, label='Test')\n",
        "plt.legend()\n",
        "plt.title(\"Distribution of target y across splits\")\n",
        "plt.show()\n",
        "\n",
        "# Q-Q plot - normal distribution https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html\n",
        "import scipy.stats as stats\n",
        "\n",
        "stats.probplot(y_train, dist=\"norm\", plot=plt)\n",
        "plt.title(\"Normal Q-Q Plot of y_train\")\n",
        "plt.show()\n",
        "\n",
        "# Skewness https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.skew.html\n",
        "from scipy.stats import skew\n",
        "\n",
        "skewness = skew(y_train)\n",
        "print(f\"Skewness of y_train: {skewness:.2f}\")\n",
        "\n",
        "# Checking if all y values are positive\n",
        "if (y_train > 0).all():\n",
        "  print(\"All positive\")\n",
        "else:\n",
        "  print(\"Some negative\")\n",
        "\n",
        "# Q-Q plot - gamma distribution\n",
        "params = stats.gamma.fit(y_train)\n",
        "stats.probplot(y_train, dist=stats.gamma, sparams=params, plot=plt)\n",
        "plt.title(\"Gamma Q-Q Plot for y_train\")\n",
        "plt.show()\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "####################################################\n",
        "# VARIANCE ACCOUNTED FOR FUNCTION\n",
        "####################################################\n",
        "\n",
        "def vaf(y_true, y_predicted):\n",
        "  return (1 - np.var(y_true - y_predicted) / np.var(y_true)) * 100\n",
        "\n",
        "\n",
        "####################################################\n",
        "# SETTING UP THE GAM MODEL, TUNING, FITTING, CHECKING RESULTS\n",
        "####################################################\n",
        "\n",
        "!pip install pygam\n",
        "\n",
        "#################### LINEAR ####################\n",
        "\n",
        "from pygam import LinearGAM, s\n",
        "from pygam.terms import TermList\n",
        "import numpy as np\n",
        "\n",
        "# Create a TermList for all predictors\n",
        "terms = TermList(*[s(i) for i in range(X_train.shape[1])])\n",
        "print(\"Terms: \", terms)\n",
        "\n",
        "# Set up GAM\n",
        "#lams = [100]*10\n",
        "lams = [[np.int64(70)], [np.int64(35)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)]]\n",
        "gam = LinearGAM(terms,lam=lams)\n",
        "gam.fit(X_train, y_train)\n",
        "\n",
        "'''\n",
        "# Tuning https://pygam.readthedocs.io/en/latest/api/gam.html#pygam.pygam.GAM.gridsearch\n",
        "\n",
        "lam = [80, 90]\n",
        "lams = [lam] * 10\n",
        "gam.gridsearch(X_train, y_train, lam=lams)\n",
        "print(gam.lam)\n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "Findings: Wery similar values of MAE, RMSE, R¬≤ for simplicity we chose lam = 100 for every s(i)\n",
        "\n",
        "========== # 1. lam = np.logspace(-2, 2, 2) ==========\n",
        "best I could do (-2,2,2), 1024 possibilities, other setup crashed, 2 options max, lam from this setup:\n",
        "[[np.float64(100.0)], [np.float64(100.0)], [np.float64(100.0)], [np.float64(100.0)], [np.float64(100.0)], [np.float64(100.0)], [np.float64(100.0)], [np.float64(100.0)], [np.float64(100.0)], [np.float64(100.0)]]\n",
        "\n",
        "Validation MAE: 9.32788483701293\n",
        "Validation RMSE: 12.337603249138263\n",
        "Validation R¬≤: 0.5356249660433803 <===\n",
        "\n",
        "Test MAE: 9.201924494177078\n",
        "Test RMSE: 13.002298117612094\n",
        "Test R¬≤: 0.5520417607555324\n",
        "\n",
        "========== #2. lam = [35, 70] ==========\n",
        "[[np.int64(70)], [np.int64(35)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)]]\n",
        "\n",
        "Validation MAE: 9.367897732609359\n",
        "Validation RMSE: 12.32485430135225\n",
        "Validation R¬≤: 0.5365841854479299 <===\n",
        "\n",
        "Test MAE: 9.328121702533512\n",
        "Test RMSE: 13.195446091358482\n",
        "Test R¬≤: 0.5386341515153062\n",
        "\n",
        "========== #3. lam = [150, 200] ==========\n",
        "[[np.int64(200)], [np.int64(150)], [np.int64(200)], [np.int64(200)], [np.int64(200)], [np.int64(200)], [np.int64(200)], [np.int64(150)], [np.int64(150)], [np.int64(200)]]\n",
        "\n",
        "Validation MAE: 9.29018662805119\n",
        "Validation RMSE: 12.350720456922284\n",
        "Validation R¬≤: 0.534637003990853 <===\n",
        "\n",
        "Test MAE: 9.143914355207844\n",
        "Test RMSE: 12.925918298047176\n",
        "Test R¬≤: 0.5572892138023167\n",
        "\n",
        "========== #4. lam = [80, 90] ==========\n",
        "[[np.int64(90)], [np.int64(80)], [np.int64(90)], [np.int64(90)], [np.int64(90)], [np.int64(90)], [np.int64(90)], [np.int64(90)], [np.int64(90)], [np.int64(90)]]\n",
        "\n",
        "Validation MAE: 9.334807841622661\n",
        "Validation RMSE: 12.329637256916634\n",
        "Validation R¬≤: 0.536224436394982 <===\n",
        "\n",
        "Test MAE: 9.233539707938771\n",
        "Test RMSE: 13.04518445874728\n",
        "Test R¬≤: 0.5490818266541244\n",
        "\n",
        "'''\n",
        "\n",
        "# gam = LinearGAM(s(0) + s(1) + s(2) + s(3) + s(4) + s(5) + s(6) + s(7) + s(8) + s(9)).fit(X_train, y_train)\n",
        "# LinearGAM(terms) same as => GAM(terms, distribution=NormalDist(), link=IdentityLink())\n",
        "\n",
        "# https://pygam.readthedocs.io/en/latest/api/api.html#generalized-additive-model-classes\n",
        "\n",
        "# gam = GammaGAM(s(0) + s(1) + f(2)) - positive skewed continuous, same as => GAM(terms, distribution=GammaDist(), link=LogLink())\n",
        "# gam = InvGaussGAM(s(0) + s(1) + f(2)) - positive, heavily skewed\n",
        "# gam = LogisticGAM(s(0) + s(1) + f(2)) - binary classification same as => GAM(terms, distribution=BinomialDist(), link=LogitLink())\n",
        "# gam = PoissonGAM(s(0) + s(1) + f(2)) - counts same as => GAM(terms, distribution=PoissonDist(), link=LogLink())\n",
        "# gam = ExpectileGAM(s(0) + s(1) + f(2))\n",
        "\n",
        "# https://pygam.readthedocs.io/en/latest/api/api.html#spline-term\n",
        "\n",
        "# gam = LinearGAM(s(0) + s(1) + s(2, n_splines=15, lam=0.7, basis=ps, spline_order=3) + s(3) + s(4) + s(5) + s(6) + s(7) + s(8) + s(9)).fit(X_train, y_train)\n",
        "# n_splines - number of splines\n",
        "# lam - overall wiggliness of entire smooth term s(i), larger lam = smoother curve - may underfit, smaller lam = more wiggly - may overfit but allows more complex relationship\n",
        "# basis - type of basis function to use in the term, pygam allows ps- p-spline basis, cp - cyclic p-spline basis\n",
        "# spline_order=3 cubic\n",
        "\n",
        "# https://app.readthedocs.org/projects/pygam/downloads/pdf/stable/\n",
        "\n",
        "# l() linear terms: for terms like ùëãùëñ, roughly straight line\n",
        "# s() spline terms: smooth, non-linear effect\n",
        "# f() factor terms: categorical variables\n",
        "# te() tensor products: Captures interaction effects between two or more variables\n",
        "# intercept: baseline value of the response when all predictors are zero\n",
        "\n",
        "\n",
        "# Display model summary\n",
        "print(\"\\n########## LINEAR GAM SUMMARY ##########\\n\")\n",
        "print(gam.summary())\n",
        "\n",
        "# Predict on validation and test sets\n",
        "#y_val_pred_l = gam.predict(X_val)\n",
        "y_test_pred_l = gam.predict(X_test)\n",
        "\n",
        "# Metrics for LinearGAM\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "#r2_val_l = r2_score(y_val, y_val_pred_l)\n",
        "r2_test_l = r2_score(y_test, y_test_pred_l)\n",
        "\n",
        "#mae_val_l = mean_absolute_error(y_val, y_val_pred_l)\n",
        "#mse_val_l = mean_squared_error(y_val, y_val_pred_l)\n",
        "#rmse_val_l = np.sqrt(mse_val_l)\n",
        "\n",
        "mae_test_l = mean_absolute_error(y_test, y_test_pred_l)\n",
        "mse_test_l = mean_squared_error(y_test, y_test_pred_l)\n",
        "rmse_test_l = np.sqrt(mse_test_l)\n",
        "\n",
        "vaf_test_l = vaf(y_test, y_test_pred_l)\n",
        "\n",
        "# print(\"Validation MAE:\", mae_val_l)\n",
        "# print(\"Validation RMSE:\", rmse_val_l)\n",
        "# print(\"Validation R¬≤:\", r2_val_l)\n",
        "print(\"\")\n",
        "print(\"Test MAE:\", mae_test_l)\n",
        "print(\"Test RMSE:\", rmse_test_l)\n",
        "print(\"Test R¬≤:\", r2_test_l)\n",
        "print(\"Test VAF:\", vaf_test_l)\n",
        "\n",
        "# Partial dependencies\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from pygam.terms import TermList\n",
        "\n",
        "\n",
        "# Partial dependence plots for each predictor (LINEAR)\n",
        "\n",
        "predictor_names = df.drop(columns=['PM10']).columns\n",
        "\n",
        "for i, name in enumerate(predictor_names):\n",
        "    XX = gam.generate_X_grid(term=i)\n",
        "    plt.figure()\n",
        "    plt.plot(XX[:, i], gam.partial_dependence(term=i, X=XX))\n",
        "    # plt.plot(XX[:, i], gam.partial_dependence(term=i, X=XX, width=0.95)[1], c='r', ls='--')\n",
        "    plt.title(f\"Partial dependence (LINEAR): {name}\")\n",
        "    plt.ylabel('PM10')\n",
        "    plt.xlabel(name)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "#################### GAMMA ###################\n",
        "\n",
        "from pygam import GAM, s\n",
        "from pygam.distributions import GammaDist\n",
        "from pygam.links import LogLink\n",
        "\n",
        "# Create TermList for all predictors\n",
        "terms = TermList(*[s(i) for i in range(X_train.shape[1])])\n",
        "\n",
        "# Fit GAM with Gamma distribution and Log link\n",
        "\n",
        "lams = [[np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(35)], [np.int64(70)], [np.int64(70)]]\n",
        "gamma_gam = GAM(terms, distribution=GammaDist(), link=LogLink(), lam=lams)\n",
        "gamma_gam.fit(X_train, y_train)\n",
        "\n",
        "'''\n",
        "# Tuning https://pygam.readthedocs.io/en/latest/api/gam.html#pygam.pygam.GAM.gridsearch\n",
        "\n",
        "# lam = np.logspace(-2, 2, 2)\n",
        "lam = [35, 70]\n",
        "lams = [lam] * X_train.shape[1]\n",
        "gamma_gam.gridsearch(X_train, y_train, lam=lams)\n",
        "print(gamma_gam.lam)\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "Findings: Wery similar values of MAE, RMSE, R¬≤, overall lower than for linear gam\n",
        "\n",
        "========== # 1. lam = np.logspace(-2, 2, 2) ==========\n",
        "[[np.float64(100.0)], [np.float64(100.0)], [np.float64(100.0)], [np.float64(100.0)], [np.float64(100.0)], [np.float64(100.0)], [np.float64(100.0)], [np.float64(100.0)], [np.float64(100.0)], [np.float64(100.0)]]\n",
        "\n",
        "Validation MAE: 9.61470659660132\n",
        "Validation RMSE: 12.934591064485462\n",
        "Validation R¬≤: 0.4895976467236781 <===\n",
        "\n",
        "Test MAE: 9.032022857406988\n",
        "Test RMSE: 12.840878748363627\n",
        "Test R¬≤: 0.5630952358238248\n",
        "\n",
        "========== #2. lam = [35, 70] ==========\n",
        "[[np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(70)], [np.int64(35)], [np.int64(70)], [np.int64(70)]]\n",
        "\n",
        "Validation MAE: 9.578659857788022\n",
        "Validation RMSE: 12.841379189145238\n",
        "Validation R¬≤: 0.49692747044373253 <===\n",
        "\n",
        "Test MAE: 9.080696917870128\n",
        "Test RMSE: 12.880167750389928\n",
        "Test R¬≤: 0.5604175666292098\n",
        "\n",
        "========== #3. lam = [150, 200] ==========\n",
        "[[np.int64(200)], [np.int64(150)], [np.int64(200)], [np.int64(200)], [np.int64(200)], [np.int64(200)], [np.int64(150)], [np.int64(150)], [np.int64(150)], [np.int64(200)]]\n",
        "\n",
        "Validation MAE: 9.59492027571841\n",
        "Validation RMSE: 12.981091818099298\n",
        "Validation R¬≤: 0.48592118616706015 <===\n",
        "\n",
        "Test MAE: 8.968009270031345\n",
        "Test RMSE: 12.780459553144002\n",
        "Test R¬≤: 0.5671970316355832\n",
        "\n",
        "========== #4. lam = [80, 90] ==========\n",
        "[[np.int64(90)], [np.int64(90)], [np.int64(90)], [np.int64(90)], [np.int64(90)], [np.int64(90)], [np.int64(90)], [np.int64(80)], [np.int64(80)], [np.int64(90)]]\n",
        "\n",
        "Validation MAE: 9.604902299089039\n",
        "Validation RMSE: 12.903763670517645\n",
        "Validation R¬≤: 0.4920276615958782 <===\n",
        "\n",
        "Test MAE: 9.048218411612211\n",
        "Test RMSE: 12.847853770595115\n",
        "Test R¬≤: 0.5626204633156002\n",
        "'''\n",
        "\n",
        "# Display model summary\n",
        "print(\"\\n########## GAMMA GAM SUMMARY ##########\\n\")\n",
        "print(gamma_gam.summary())\n",
        "\n",
        "# Predict on validation and test sets\n",
        "#y_val_pred_g = gamma_gam.predict(X_val)\n",
        "y_test_pred_g = gamma_gam.predict(X_test)\n",
        "\n",
        "# Metrics for GammaGAM\n",
        "\n",
        "#r2_val_g = r2_score(y_val, y_val_pred_g)\n",
        "r2_test_g = r2_score(y_test, y_test_pred_g)\n",
        "\n",
        "#mae_val_g = mean_absolute_error(y_val, y_val_pred_g)\n",
        "#mse_val_g = mean_squared_error(y_val, y_val_pred_g)\n",
        "#rmse_val_g = np.sqrt(mse_val_g)\n",
        "\n",
        "mae_test_g = mean_absolute_error(y_test, y_test_pred_g)\n",
        "mse_test_g = mean_squared_error(y_test, y_test_pred_g)\n",
        "rmse_test_g = np.sqrt(mse_test_g)\n",
        "\n",
        "vaf_test_g = vaf(y_test, y_test_pred_g)\n",
        "\n",
        "# print(\"Validation MAE:\", mae_val_g)\n",
        "# print(\"Validation RMSE:\", rmse_val_g)\n",
        "# print(\"Validation R¬≤:\", r2_val_g)\n",
        "print(\"\")\n",
        "print(\"Test MAE:\", mae_test_g)\n",
        "print(\"Test RMSE:\", rmse_test_g)\n",
        "print(\"Test R¬≤:\", r2_test_g)\n",
        "print(\"Test VAF:\", vaf_test_g)\n",
        "\n",
        "# Partial dependence plots for each predictor (GAMMA)\n",
        "\n",
        "predictor_names = df.drop(columns=['PM10']).columns\n",
        "\n",
        "for i, name in enumerate(predictor_names):\n",
        "    XX = gamma_gam.generate_X_grid(term=i)\n",
        "    plt.figure()\n",
        "    plt.plot(XX[:, i], gamma_gam.partial_dependence(term=i, X=XX))\n",
        "    # plt.plot(XX[:, i], gam.partial_dependence(term=i, X=XX, width=0.95)[1], c='r', ls='--')\n",
        "    plt.title(f\"Partial dependence (GAMMA): {name}\")\n",
        "    plt.ylabel('PM10')\n",
        "    plt.xlabel(name)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "####################################################\n",
        "# LINEAR REGRESSION\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
        "####################################################\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "print(\"\\n########## LINEAR REGRESSION SUMMARY ##########\\n\")\n",
        "\n",
        "\n",
        "linear_reg = LinearRegression().fit(X_train, y_train)\n",
        "print(\"Coefficients: \", linear_reg.coef_)\n",
        "print(\"Intercept: \",linear_reg.intercept_)\n",
        "\n",
        "# Predict on validation and test sets\n",
        "# y_val_pred_lr = linear_reg.predict(X_val)\n",
        "y_test_pred_lr = linear_reg.predict(X_test)\n",
        "\n",
        "# Metrics for Linear Regression\n",
        "\n",
        "# r2_val_lr = r2_score(y_val, y_val_pred_lr)\n",
        "r2_test_lr = r2_score(y_test, y_test_pred_lr)\n",
        "\n",
        "# mae_val_lr = mean_absolute_error(y_val, y_val_pred_lr)\n",
        "# mse_val_lr = mean_squared_error(y_val, y_val_pred_lr)\n",
        "# rmse_val_lr = np.sqrt(mse_val_lr)\n",
        "\n",
        "mae_test_lr = mean_absolute_error(y_test, y_test_pred_lr)\n",
        "mse_test_lr = mean_squared_error(y_test, y_test_pred_lr)\n",
        "rmse_test_lr = np.sqrt(mse_test_lr)\n",
        "\n",
        "vaf_test_lr = vaf(y_test, y_test_pred_lr)\n",
        "\n",
        "# print(\"Validation MAE:\", mae_val_lr)\n",
        "# print(\"Validation RMSE:\", rmse_val_lr)\n",
        "# print(\"Validation R¬≤:\", r2_val_lr)\n",
        "# print(linear_reg.score(X_val,y_val)) # same as R^2\n",
        "print(\"\")\n",
        "print(\"Test MAE:\", mae_test_lr)\n",
        "print(\"Test RMSE:\", rmse_test_lr)\n",
        "print(\"Test R¬≤:\", r2_test_lr)\n",
        "print(\"Test VAF:\", vaf_test_lr)\n",
        "\n",
        "print(linear_reg.score(X_test,y_test)) # same as R^2\n",
        "\n",
        "####################################################\n",
        "# RANDOM FOREST\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
        "####################################################\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "print(\"\\n########## RANDOM FOREST SUMMARY ##########\\n\")\n",
        "\n",
        "#random_forest = RandomForestRegressor().fit(X_train, y_train)\n",
        "\n",
        "# RANDOM FOREST (TUNED)\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Tune hyperparameters for better performance\n",
        "random_forest = RandomForestRegressor(\n",
        "    n_estimators=300,       # number of trees\n",
        "    max_depth=10,           # controls tree depth to reduce overfitting\n",
        "    min_samples_split=5,    # minimum samples required to split a node\n",
        "    min_samples_leaf=3,     # minimum samples at a leaf node\n",
        "    random_state=18,        # for reproducibility\n",
        "    n_jobs=-1               # uses all CPU cores\n",
        ").fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Predict on validation and test sets\n",
        "# y_val_pred_rf = random_forest.predict(X_val)\n",
        "y_test_pred_rf = random_forest.predict(X_test)\n",
        "\n",
        "# Metrics for Random Forest\n",
        "\n",
        "# r2_val_rf = r2_score(y_val, y_val_pred_rf)\n",
        "r2_test_rf = r2_score(y_test, y_test_pred_rf)\n",
        "\n",
        "# mae_val_rf = mean_absolute_error(y_val, y_val_pred_rf)\n",
        "# mse_val_rf = mean_squared_error(y_val, y_val_pred_rf)\n",
        "# rmse_val_rf = np.sqrt(mse_val_rf)\n",
        "\n",
        "mae_test_rf = mean_absolute_error(y_test, y_test_pred_rf)\n",
        "mse_test_rf = mean_squared_error(y_test, y_test_pred_rf)\n",
        "rmse_test_rf = np.sqrt(mse_test_rf)\n",
        "\n",
        "vaf_test_rf = vaf(y_test, y_test_pred_rf)\n",
        "\n",
        "# print(\"Validation MAE:\", mae_val_rf)\n",
        "# print(\"Validation RMSE:\", rmse_val_rf)\n",
        "# print(\"Validation R¬≤:\", r2_val_rf)\n",
        "# print(random_forest.score(X_val,y_val)) # same as R^2\n",
        "print(\"\")\n",
        "print(\"Test MAE:\", mae_test_rf)\n",
        "print(\"Test RMSE:\", rmse_test_rf)\n",
        "print(\"Test R¬≤:\", r2_test_rf)\n",
        "print(\"Test VAF:\", vaf_test_rf)\n",
        "print(random_forest.score(X_test,y_test)) # same as R^2\n",
        "\n",
        "\n",
        "####################################################\n",
        "# MODEL COMPARISON SUMMARY TABLE\n",
        "####################################################\n",
        "import pandas as pd\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    'Model': ['LinearGAM', 'GammaGAM', 'LinearRegression', 'RandomForest'],\n",
        "#    'Validation R¬≤': [r2_val_l, r2_val_g, r2_val_lr, r2_val_rf],\n",
        "    'Test R¬≤': [r2_test_l, r2_test_g, r2_test_lr, r2_test_rf],\n",
        "    'Test RMSE': [rmse_test_l, rmse_test_g, rmse_test_lr, rmse_test_rf],\n",
        "    'Test MAE': [mae_test_l, mae_test_g, mae_test_lr, mae_test_rf],\n",
        "    'Test VAF %': [vaf_test_l, vaf_test_g, vaf_test_lr, vaf_test_rf]\n",
        "})\n",
        "\n",
        "print(\"\\n########## MODEL COMPARISON ##########\\n\")\n",
        "display(results)\n",
        "\n",
        "\n",
        "####################################################\n",
        "# PREDICTED VS ACTUAL VALUES\n",
        "####################################################\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "### LINEAR GAM, GAMMA GAM, RANDOM FOREST AND LINEAR REGRESSION ALL SEPARATE\n",
        "\n",
        "models = ['Linear GAM', 'Gamma GAM', 'Linear Regression', 'Random Forest']\n",
        "predictions = [y_test_pred_l, y_test_pred_g, y_test_pred_lr, y_test_pred_rf]\n",
        "vafs = [vaf_test_l, vaf_test_g, vaf_test_lr, vaf_test_rf]\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(14, 8))\n",
        "\n",
        "for i, ax in enumerate(axs.flatten()):\n",
        "  ax.scatter(y_test, predictions[i], alpha=0.6, label=models[i])\n",
        "  ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "  ax.set_xlabel(\"Actual PM10\")\n",
        "  ax.set_ylabel(\"Predicted PM10\")\n",
        "  ax.set_title(f\"Predicted vs Actual PM10 Concentration - {models[i]}, VAF = {vafs[i]:.2f}%\" )\n",
        "  ax.legend()\n",
        "  ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "### LINEAR GAM, RANDOM FOREST AND LINEAR REGRESSION\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_test, y_test_pred_rf, alpha=0.6, label='Random Forest')\n",
        "plt.scatter(y_test, y_test_pred_lr, alpha=0.6, label='Linear Regression')\n",
        "plt.scatter(y_test, y_test_pred_l, alpha=0.6, label='Linear GAM')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.xlabel(\"Actual PM10\")\n",
        "plt.ylabel(\"Predicted PM10\")\n",
        "plt.title(\"Predicted vs Actual PM10 Concentration\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### GAM: LINEAR AND GAMMA\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_test, y_test_pred_l, alpha=0.6, label='Linear GAM')\n",
        "plt.scatter(y_test, y_test_pred_g, alpha=0.6, label='Gamma GAM')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.xlabel(\"Actual PM10\")\n",
        "plt.ylabel(\"Predicted PM10\")\n",
        "plt.title(\"Predicted vs Actual PM10 Concentration\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "####################################################\n",
        "# RESIDUAL PLOT (ERROR ANALYSIS)\n",
        "####################################################\n",
        "residuals = y_test - y_test_pred_rf\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(y_test_pred_rf, residuals, alpha=0.6, color='purple', edgecolor='black')\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title(\"Residual Plot for Random Forest\")\n",
        "plt.xlabel(\"Predicted PM10\")\n",
        "plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
        "plt.show()\n",
        "\n",
        "# Optional: summary statistics for residuals\n",
        "import numpy as np\n",
        "print(\"Mean of residuals:\", np.mean(residuals))\n",
        "print(\"Standard deviation of residuals:\", np.std(residuals))\n",
        "\n",
        "\n",
        "\n",
        "####################################################\n",
        "# FEATURE IMPORTANCE (RANDOM FOREST)\n",
        "####################################################\n",
        "import numpy as np\n",
        "\n",
        "importances = random_forest.feature_importances_\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(range(len(importances)), importances)\n",
        "plt.title(\"Random Forest Feature Importances\")\n",
        "plt.xlabel(\"Feature Index (X1‚ÄìX10)\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "####################################################\n",
        "# GAM PARTIAL DEPENDENCE PLOTS (INTERPRETATION)\n",
        "####################################################\n",
        "fig, axs = plt.subplots(5, 2, figsize=(12, 12))\n",
        "for i, ax in enumerate(axs.flatten()):\n",
        "    XX = gam.generate_X_grid(term=i)\n",
        "    ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX))\n",
        "    ax.set_title(f\"Partial dependence for feature {i+1}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "####################################################\n",
        "# RANDOM FOREST CROSS-VALIDATION\n",
        "####################################################\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(random_forest, X, y, cv=5, scoring='r2')\n",
        "print(\"Average 5-Fold CV R¬≤ for Random Forest:\", scores.mean())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}